{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fcc6f8f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:16.516307Z",
     "iopub.status.busy": "2023-10-25T15:58:16.515427Z",
     "iopub.status.idle": "2023-10-25T15:58:20.321910Z",
     "shell.execute_reply": "2023-10-25T15:58:20.320533Z"
    },
    "papermill": {
     "duration": 3.817026,
     "end_time": "2023-10-25T15:58:20.325285",
     "exception": false,
     "start_time": "2023-10-25T15:58:16.508259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47ac7e",
   "metadata": {
    "papermill": {
     "duration": 0.003922,
     "end_time": "2023-10-25T15:58:20.336202",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.332280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37f8096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.347391Z",
     "iopub.status.busy": "2023-10-25T15:58:20.346762Z",
     "iopub.status.idle": "2023-10-25T15:58:20.357814Z",
     "shell.execute_reply": "2023-10-25T15:58:20.356478Z"
    },
    "papermill": {
     "duration": 0.019484,
     "end_time": "2023-10-25T15:58:20.360369",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.340885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def assert_allclose(a, b, atol=1e-5, rtol=1e-5):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return np.testing.assert_allclose(a, b, atol=atol, rtol=rtol)\n",
    "\n",
    "def allclose(a, b, atol=1e-5, rtol=1e-5):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return np.allclose(a, b, atol=atol, rtol=rtol)\n",
    "\n",
    "def compute_thetas(head_dim, device=\"cpu\", base=10000.0):\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    thetas = 1.0 / (base ** (theta_numerator / head_dim)).to(device)\n",
    "    return thetas\n",
    "\n",
    "def my_compute_thetas(head_dim, device=\"cpu\", base=10000.0):\n",
    "    \"\"\"\n",
    "    theta_i = 10000^{-2(i-1)/dim} for i in [1, 2, ..., dim/2]\n",
    "    \"\"\"\n",
    "    assert head_dim % 2 == 0\n",
    "    exponents = [-2 * (i-1) / head_dim for i in range(1, head_dim//2 + 1)]\n",
    "    exponents = torch.tensor(exponents, dtype=torch.float).to(device)\n",
    "    thetas = base ** exponents\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a355e201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.371959Z",
     "iopub.status.busy": "2023-10-25T15:58:20.370554Z",
     "iopub.status.idle": "2023-10-25T15:58:20.449856Z",
     "shell.execute_reply": "2023-10-25T15:58:20.448068Z"
    },
    "papermill": {
     "duration": 0.088106,
     "end_time": "2023-10-25T15:58:20.452822",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.364716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = compute_thetas(128)\n",
    "b = my_compute_thetas(128)\n",
    "print(allclose(a, b))\n",
    "# assert_allclose(a + 1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c284b82",
   "metadata": {
    "papermill": {
     "duration": 0.004356,
     "end_time": "2023-10-25T15:58:20.461821",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.457465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09e9ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.473398Z",
     "iopub.status.busy": "2023-10-25T15:58:20.472932Z",
     "iopub.status.idle": "2023-10-25T15:58:20.520332Z",
     "shell.execute_reply": "2023-10-25T15:58:20.519110Z"
    },
    "papermill": {
     "duration": 0.0571,
     "end_time": "2023-10-25T15:58:20.523336",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.466236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(\n",
    "    head_dim: int, \n",
    "    seq_len: int,\n",
    "    device: str,\n",
    "    base: float = 10000.0\n",
    "):\n",
    "    \"\"\"\n",
    "    The formula: theta_i = 10000^{-2(i-1)/dim} for i in [1, 2, ..., dim/2]\n",
    "    Simplified a little bit: a_i = 10000^{2(i-1)/dim}, theta_i = 1/a\n",
    "\n",
    "    To rotate a pair, we need to know the position of that pair in the vector embedding (i for i in [0, 1, ..., d/2]), and the position of that vector embedding in the sequence of tokens ([j for j in [0, 1, ..., seq_len]]). \n",
    "    p_{i, j} = m*theta. \n",
    "    where m = M[j] and theta = Theta[i]\n",
    "    This function basically precompute all the p_{i, j} (in the complex form). \n",
    "\n",
    "    Theta = [θ_0, θ_1, ..., θ_{d/2}]\n",
    "    M = [0, 1, ..., seq_len - 1]\n",
    "    So basically to compute all possible p_{i, j} we need to compute outer product of Theta and M.\n",
    "\n",
    "    \"\"\"\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    \n",
    "    # Build the theta parameters\n",
    "    # According to the formula theta_i = 10000^{-2(i-1)/dim} for i in [1, 2, ..., dim/2]\n",
    "    # Shape (head_dim/2,)\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (base ** (theta_numerator / head_dim)).to(device)\n",
    "    \n",
    "    # Construct the positions (the \"m\" parameters)\n",
    "    # Shape: (seq_len,)\n",
    "    m = torch.arange(seq_len).to(device)\n",
    "    \n",
    "    # Multiply each theta by each position using outer product. freqs below is like the angle for ratation\n",
    "    # Shape: m * theta: (seq_len,) outer (head_dim/2,) -> (seq_len, head_dim/2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    " \n",
    "    # Compute complex numbers in the polar form: c = R * exp(m * theta) ~ cos(m * theta) + i * sin(m * theta)\n",
    "    # where R = 1, as follows:\n",
    "    # (seq_len, head_dim/2) -> (seq_len, head_dim/2)\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "    \n",
    "def apply_rotary_embeddings(\n",
    "    x: torch.Tensor, \n",
    "    freqs_complex: torch.Tensor, \n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    For a vector of dim D, to apply rotation to it, we need to specify the thetas.\n",
    "    It's like, according to the paper and my own interpretation, we need to first \n",
    "    1. chunk the vector into pairs: \n",
    "    [v_0, v_1, ..., v_{d-1}] -> [[v_0, v_1], ..., [v_{d-2}, v_{d-1}]]\n",
    "    2. rotate pairs by pairs\n",
    "    -> [rotate([v_0, v_1]), ..., rotate([v_{d-2}, v_{d-1}])]\n",
    "    3. squeeze all the rotated pairs and concatenate them back to a vector. \n",
    "    -> [v_0', v_1', ..., v_{d-1}'] where v_i' is rotated v_i.\n",
    "\n",
    "    It's like a lazy rotation to me. Instead of rotate the vector as a whole, we rotate partitions of it (in the form of pairs), one by one. \n",
    "    The key thing here, to rotate a pair, we need a precomputed angle accociated to it. The following code will solve this.\n",
    "    Args:\n",
    "        x: input tensor of shape (B, seq_len, n_heads, head_dim)\n",
    "        freqs_complex: precomputed theta position frequencies, tensor of shape (seq_len, head_dim/2)\n",
    "        *note*: When pplying kv_cache for inference, x is a tensor with seq_len = 1. freqs_complex is then a tensor of size (1, head_dim/2). This freqs_complex needs to be extracted from the full precompted freqs complex matrix, at the position that aligns with x. For example, when the position of x is `start_pos`, to retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + 1]:\n",
    "        freqs_complex = full_freqs_complex[start_pos:start_pos + 1]\n",
    "        \n",
    "    Outputs:\n",
    "        x_out: rotated tensor, of shape (B, seq_len, n_heads, head_dim)\n",
    "    \"\"\"\n",
    "    # (B, seq_len, n_heads, head_dim) -> (B, seq_len, n_heads, head_dim/2, 2)\n",
    "    x_complex = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    # (B, seq_len, n_heads, head_dim/2, 2) -> (B, seq_len, n_heads, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x_complex)\n",
    "    # (seq_len, head_dim/2) -> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # broadcast: (B, seq_len, n_heads, head_dim/2) * (1, seq_len, 1, head_dim/2)\n",
    "    # -> (B, seq_len, n_heads, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # (B, seq_len, n_heads, head_dim/2) -> (B, seq_len, n_heads, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, seq_len, n_heads, head_dim/2, 2) -> (B, seq_len, n_heads, head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1: return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32 # Number of heads for queries. like (1, 4096) -> (1, 32, 128)\n",
    "    n_kv_heads: Optional[int] = None # Numer of heads for K and V. (?) Is this for grouped query attention?\n",
    "    vocab_size: int = -1 # This will bet set when we load the tokenizer \n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    # Needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = \"cpu\"\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # The gamma parameter\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # formula: a_i_normed = a_i / RMS(a). RMS(a) = sqrt(1/dim * sum(a^2)) \n",
    "        # (B, seq_len, dim) * (B, seq_len, 1) -> (B, seq_len, dim)\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        a = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(a + self.eps)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (dim) * (B, seq_len, dim) = (B, seq_len, dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_heads_q = args.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads # indicate how many times the keys and values should be repeated\n",
    "        self.head_dim = args.dim // args.n_heads \n",
    "        assert self.n_rep * self.n_kv_heads == self.n_heads_q \n",
    "        assert self.head_dim * args.n_heads == args.dim\n",
    "        assert self.n_kv_heads * self.head_dim == args.dim / self.n_rep\n",
    "        \n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False) ## equivalent to nn.Linear(args.dim, args.dim)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False) ## equivalent to nn.Linear(args.dim, args.dim / self.n_rep)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False) ## equivalent to nn.Linear(args.dim, args.dim / self.n_rep)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False) ## equivalent to nn.Linear(args.dim, args.dim)\n",
    "        \n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, # (B, 1, dim). here seq_len = 1\n",
    "        start_pos: int,\n",
    "        freqs_complex: torch.Tensor # (1, head_dim/2). this is extracted from the full freqs_complex\n",
    "    ):\n",
    "        batch_size, seq_len, _ = x.shape # (B, 1, dim)\n",
    "        assert seq_len == 1\n",
    "        xq = self.wq(x) # (B, 1, dim) -> (B, 1, H_Q, head_dim)\n",
    "        xk = self.wk(x) # (B, 1, dim) -> (B, 1, H_KV, head_dim)\n",
    "        xv = self.wv(x) # (B, 1, dim) -> (B, 1, H_KV, head_dim)\n",
    "        \n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        ## Apply rotary embedding to q and k only\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
    "        \n",
    "        ## Update the KV cache\n",
    "        self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk # (B, 1, H_KV, head_dim)\n",
    "        self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
    "        \n",
    "        ## Extract neccessary cached keys and values: all upto the current just computed key and value\n",
    "        keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "        \n",
    "        ## Repeat keys and values to match the size of queries. Make this standard multihead attention.\n",
    "        keys = repeat_kv(keys, self.n_rep) # (B, seq_len_KV, H_KV, head_dim) --> (B, seq_len_KV, H_Q, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep) # (B, seq_len_KV, H_KV, head_dim) --> (B, seq_len_KV, H_Q, head_dim)\n",
    "        \n",
    "        ## Transpose for matmul compability\n",
    "        xq = xq.transpose(1, 2) # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        keys = keys.transpose(1, 2) # (B, seq_len_KV, H_Q, head_dim) -> (B, H_Q, seq_len_KV, head_dim)\n",
    "        values = values.transpose(1, 2) # (B, seq_len_KV, H_Q, head_dim) -> (B, H_Q, seq_len_KV, head_dim)\n",
    "        \n",
    "        # (B, H_Q, 1, head_dim) @ (B, H_Q, head_dim, seq_len_KV) -> (B, H_Q, 1, seq_len_KV)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, seq_len_KV) -> (B, H_Q, 1, seq_len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # (B, H_Q, 1, seq_len_KV) @ (B, H_Q, seq_len_KV, head_dim) -> (B, H_Q, 1, head_dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        ## Concat output heads\n",
    "        assert self.n_heads_q * self.head_dim == self.args.dim\n",
    "        # (B, H_Q, 1, head_dim) -> (B, 1, dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1) # last dim: self.n_heads_q * self.head_dim\n",
    "        assert output.shape[-1] == self.n_heads_q * self.head_dim\n",
    "        output = self.wo(output) # (B, 1, dim) -> (B, 1, dim)\n",
    "        return output\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        shape transformation: dim -> hidden_dim -> dim.\n",
    "        Vanilla: FFN(x) = ReLU(xW1 + b1)W2 + b2 \n",
    "        SwiGLU: FFN(x) = (Swish(xW1) ⊙ xW3)W2\n",
    "        \"\"\"\n",
    "        swish = F.silu(self.w1(x)) # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x) # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = self.w2(x) # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads \n",
    "        self.dim = args.dim \n",
    "        self.head_dim = args.dim // args.n_heads \n",
    "        \n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "        \n",
    "        # Normalize BEFORE the attention block\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # Normalize BEFORE the feedforward block\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        # (B, seq_len, dim) + (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_complex\n",
    "        )\n",
    "        # (B, seq_len, dim) + (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(args) for _ in range(args.n_layers)]\n",
    "        )\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
    "            self.args.dim // self.args.n_heads,\n",
    "            self.args.max_seq_len * 2,\n",
    "            device=self.args.device\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        \"\"\"\n",
    "        Here we use KV cache, therefore only one token is needed to compute the next token.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        assert seq_len == 1, \"Only 1 token\"\n",
    "\n",
    "        # (B, seq_len) -> (B, seq_len, dim)\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        # Retrieve te pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62247acf",
   "metadata": {
    "papermill": {
     "duration": 0.004107,
     "end_time": "2023-10-25T15:58:20.532025",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.527918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d1b3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.542784Z",
     "iopub.status.busy": "2023-10-25T15:58:20.542308Z",
     "iopub.status.idle": "2023-10-25T15:58:20.569649Z",
     "shell.execute_reply": "2023-10-25T15:58:20.568046Z"
    },
    "papermill": {
     "duration": 0.036506,
     "end_time": "2023-10-25T15:58:20.572711",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.536205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b8765c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.584258Z",
     "iopub.status.busy": "2023-10-25T15:58:20.583811Z",
     "iopub.status.idle": "2023-10-25T15:58:20.608887Z",
     "shell.execute_reply": "2023-10-25T15:58:20.607598Z"
    },
    "papermill": {
     "duration": 0.034451,
     "end_time": "2023-10-25T15:58:20.611863",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.577412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Llama:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Transformer, \n",
    "        tokenizer: SentencePieceProcessor, \n",
    "        model_args: ModelArgs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(\n",
    "        checkpoints_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        load_model: bool,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        device: str\n",
    "    ):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob('*.pth'))\n",
    "            assert len(checkpoints) > 0, \"No checkpoints files found\"\n",
    "            chk_path = checkpoints[0]\n",
    "            print(f\"Loading checkpoint {chk_path}\")\n",
    "            checkpoint = torch.load(chk_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {(time.time() - prev_time):.2f}s\")\n",
    "            prev_time = time.time()\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "        \n",
    "        model_args = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        elif device == \"cpu\":\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = Transformer(model_args).to(device)\n",
    "        \n",
    "        if load_model:\n",
    "            del checkpoint['rope.freqs']\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
    "        \n",
    "        return Llama(model, tokenizer, model_args)\n",
    "    \n",
    "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "        # Convert each prompt into tokens\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the maximum sequence length\n",
    "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # Populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # Greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # Only replace token if it is a padding token\n",
    "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # Cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "    \n",
    "    def _sample_top_p(self, probs, p):\n",
    "        # (B, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        # (B, vocab_size)\n",
    "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p \n",
    "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "        probs_sort[mask] = 0.0 \n",
    "        # Redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # Sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # Get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token) \n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97c0ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T15:58:20.623261Z",
     "iopub.status.busy": "2023-10-25T15:58:20.622796Z",
     "iopub.status.idle": "2023-10-25T16:02:03.263653Z",
     "shell.execute_reply": "2023-10-25T16:02:03.259608Z"
    },
    "papermill": {
     "duration": 222.653569,
     "end_time": "2023-10-25T16:02:03.270035",
     "exception": false,
     "start_time": "2023-10-25T15:58:20.616466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint /kaggle/input/llama-2/pytorch/7b/1/consolidated.00.pth\n",
      "Loaded checkpoint in 123.01s\n",
      "Loaded state dict in 98.77s\n"
     ]
    }
   ],
   "source": [
    "model_path = '/kaggle/input/llama-2/pytorch/7b/1/'\n",
    "tokenizer_path = '/kaggle/input/llama-2/pytorch/7b/1/tokenizer.model'\n",
    "torch.manual_seed(0)\n",
    "\n",
    "allow_cuda = False\n",
    "device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
    "model = Llama.build(\n",
    "    checkpoints_dir=model_path,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a860de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T16:02:03.292716Z",
     "iopub.status.busy": "2023-10-25T16:02:03.291613Z",
     "iopub.status.idle": "2023-10-25T16:44:28.335381Z",
     "shell.execute_reply": "2023-10-25T16:44:28.333670Z"
    },
    "papermill": {
     "duration": 2545.058526,
     "end_time": "2023-10-25T16:44:28.338099",
     "exception": false,
     "start_time": "2023-10-25T16:02:03.279573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 110/110 [42:24<00:00, 23.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) time is relative to the observer, 2) mass is relative to the observer, 3) speed is relative to the observer, and 4) energy is relative to the observer.ergy is relative to the observer.\n",
      "In 1905, Albert Einstein published a paper called \"On the Electrodynamics of Moving Bodies\" in which he proposed that the speed of light is a constant for all observers, and that the laws of\n",
      "--------------------------------------------------\n",
      "If Google was an Italian company founded in Milan, it would be the 4th largest company in Italy. duh!\n",
      "Google is a global company. Google has its headquarters in California, but its employees are all over the world. Google has offices in 38 countries.\n",
      "If Google was an Italian company founded in Milan, it would be the 4th largest company in Italy.\n",
      "Google is a global company. Google has its headquarters in California, but its employees are all over the world. Google has offices in 38\n",
      "--------------------------------------------------\n",
      "Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "peppermint => menthe poivrée\n",
      "plush girafe => girafe peluche\n",
      "cheese => fromage\n",
      "cactus => cactus\n",
      "\n",
      " #ifndef _ST_H\n",
      "#define _ST_H\n",
      "\n",
      "#include <sys/types.h>\n",
      "#include <sys/stat.h>\n",
      "\n",
      "#ifdef __cplusplus\n",
      "extern \"C\n",
      "--------------------------------------------------\n",
      "Tell me if the following person is actually Doraemon disguised as human:\n",
      "Name: Umar Jamil\n",
      "Decision: \n",
      "    1. He has a cat that he keeps in a box.\n",
      "    2. He has a robot that he keeps in a box.\n",
      "    3. He has a friend that he keeps in a box.\n",
      "    4. He has a magic pencil that he keeps in a box.\n",
      "    5. He has a dog that he keeps in a box.\n",
      "    6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "peppermint => menthe poivrée\n",
    "plush girafe => girafe peluche\n",
    "cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
    "Name: Umar Jamil\n",
    "Decision: \n",
    "    \"\"\"\n",
    "]\n",
    "out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f'{out_texts[i]}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7b17e",
   "metadata": {
    "papermill": {
     "duration": 0.011434,
     "end_time": "2023-10-25T16:44:28.361577",
     "exception": false,
     "start_time": "2023-10-25T16:44:28.350143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Misc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2778.855405,
   "end_time": "2023-10-25T16:44:30.713229",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-25T15:58:11.857824",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
